{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/Machine_Learning_course_UGent_D012554_2025/blob/main/notebooks/mnist_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZG3MivQWKMv"
      },
      "source": [
        "# MNIST Digit Classification with Logistic Regression and Validation\n",
        "\n",
        "This notebook demonstrates how to build a simple logistic regression model to classify handwritten digits from the MNIST dataset using PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G828FD_qWKMw"
      },
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "First, we'll import the necessary libraries and set a random seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJbZZimnWKMx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models.vision_transformer import VisionTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73JCcImlWKMy"
      },
      "source": [
        "### Libraries Used:\n",
        "\n",
        "- **torch**: The main PyTorch library for tensor operations and neural network building blocks\n",
        "- **torch.nn**: Contains neural network layers, loss functions, and other components\n",
        "- **torch.optim**: Provides optimization algorithms like SGD, Adam, etc.\n",
        "- **torch.utils.data**: Contains utilities for data loading and batching, including the `random_split` function for creating validation sets\n",
        "- **torchvision**: Provides datasets, model architectures, and image transformations for computer vision\n",
        "- **matplotlib.pyplot**: For visualization and plotting\n",
        "- **numpy**: For numerical operations\n",
        "\n",
        "We set a random seed to ensure reproducibility of our results. This means that if someone else runs this code with the same seed, they should get identical results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRBx7SYJWKMz"
      },
      "source": [
        "## Loading and Preprocessing the MNIST Dataset\n",
        "\n",
        "The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9). It contains 60,000 training images and 10,000 test images.\n",
        "\n",
        "We'll use torchvision's built-in datasets module to download and load the data. We'll also split the training data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0oYLs52WKMz"
      },
      "outputs": [],
      "source": [
        "# Define data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors and normalize pixel values to [0, 1]\n",
        "])\n",
        "\n",
        "# Download and load the full training data\n",
        "full_train_dataset = datasets.MNIST(\n",
        "    root='./data',          # Directory where the data will be stored\n",
        "    train=True,             # Use the training split\n",
        "    download=True,          # Download the data if it's not already downloaded\n",
        "    transform=transform     # Apply the defined transformations\n",
        ")\n",
        "\n",
        "validation_split = 0.1  # 10% of training data for validation\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "val_size = int(len(full_train_dataset) * validation_split)\n",
        "train_size = len(full_train_dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Training set size: {train_size}\")\n",
        "print(f\"Validation set size: {val_size}\")\n",
        "\n",
        "# Download and load test data\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',          # Directory where the data will be stored\n",
        "    train=False,            # Use the test split\n",
        "    download=True,          # Download the data if it's not already downloaded\n",
        "    transform=transform     # Apply the defined transformations\n",
        ")\n",
        "\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeQpSi7eWKMz"
      },
      "source": [
        "### Data Preprocessing Explanation:\n",
        "\n",
        "1. **transforms.Compose**: Combines multiple transforms together. In this case, we're only using one transform, but in more complex scenarios, you might apply multiple transformations like resizing, cropping, normalization, etc.\n",
        "\n",
        "2. **transforms.ToTensor()**: Converts PIL images or NumPy arrays to PyTorch tensors. It also scales the pixel values from [0, 255] to [0, 1], which is a common preprocessing step for neural networks.\n",
        "\n",
        "3. **datasets.MNIST**: This is a built-in dataset class in torchvision that handles downloading and loading the MNIST dataset.\n",
        "   - `root='./data'`: Specifies where to store the dataset files\n",
        "   - `train=True/False`: Whether to use the training or test split\n",
        "   - `download=True`: Automatically download the dataset if it's not already present\n",
        "   - `transform=transform`: Apply the defined transformations to the images\n",
        "\n",
        "4. **random_split**: Splits a dataset into non-overlapping new datasets of given lengths.\n",
        "   - We calculate the validation set size as a percentage of the full training dataset\n",
        "   - We split the full training dataset into training and validation sets\n",
        "   - This gives us a way to monitor the model's performance on unseen data during training\n",
        "\n",
        "The MNIST dataset is already split into training and test sets. We further split the training set into training and validation sets. The training set is used to train the model, the validation set is used to monitor performance during training, and the test set is used to evaluate the final model's performance on completely unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbldJTwJWKMz"
      },
      "source": [
        "## Creating Data Loaders\n",
        "\n",
        "Data loaders handle batching, shuffling, and loading the data in parallel using multiple workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3-QgEyeWKM0"
      },
      "outputs": [],
      "source": [
        "batch_size = 32        # Number of samples processed before the model is updated\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,  # The dataset to load from\n",
        "    batch_size=batch_size,  # How many samples per batch\n",
        "    shuffle=True            # Whether to shuffle the data\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,    # The dataset to load from\n",
        "    batch_size=batch_size,  # How many samples per batch\n",
        "    shuffle=False           # No need to shuffle validation data\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,   # The dataset to load from\n",
        "    batch_size=batch_size,  # How many samples per batch\n",
        "    shuffle=False           # No need to shuffle test data\n",
        ")\n",
        "\n",
        "# Calculate how many batches we have\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUKDYihTWKM0"
      },
      "source": [
        "### Data Loader Explanation:\n",
        "\n",
        "The `DataLoader` class provides an efficient way to iterate through the dataset in batches during training and evaluation.\n",
        "\n",
        "- **dataset**: The dataset from which to load the data\n",
        "- **batch_size**: How many samples to load per batch\n",
        "- **shuffle**: Whether to shuffle the data at the start of each epoch\n",
        "  - For the training data, we set `shuffle=True` to ensure that the model sees the data in a different order each epoch, which helps prevent the model from memorizing the order of the samples\n",
        "  - For the validation and test data, we set `shuffle=False` because the order doesn't matter for evaluation\n",
        "\n",
        "We create three data loaders:\n",
        "1. **train_loader**: For training the model\n",
        "2. **val_loader**: For validating the model during training\n",
        "3. **test_loader**: For final evaluation of the model\n",
        "\n",
        "The DataLoader will automatically handle the batching of data, which means it will group the data into batches of size `batch_size` and provide an iterator to go through these batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndtoAHkNWKM0"
      },
      "source": [
        "## Visualizing Training Examples\n",
        "\n",
        "Let's visualize some examples from the training dataset to get a better understanding of what our model will be working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-76jfOStWKM0"
      },
      "outputs": [],
      "source": [
        "# Function to visualize examples from the dataset\n",
        "def show_examples(dataset, num_examples=5):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(num_examples):\n",
        "        # For datasets wrapped in random_split, we need to access the dataset differently\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            img, label = dataset.dataset[dataset.indices[i]]\n",
        "        else:\n",
        "            img, label = dataset[i]\n",
        "        plt.subplot(1, num_examples, i+1)  # Create a subplot in a 1 x num_examples grid\n",
        "        plt.imshow(img.squeeze().numpy(), cmap='gray')  # Display the image in grayscale\n",
        "        plt.title(f'Label: {label}')       # Set the title to the label\n",
        "        plt.axis('off')                    # Hide the axes\n",
        "    plt.tight_layout()                     # Adjust the spacing between subplots\n",
        "    plt.show()\n",
        "\n",
        "# Show 5 examples from the training dataset\n",
        "show_examples(train_dataset, num_examples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwAUiSDwWKM0"
      },
      "source": [
        "### Visualization Function Explanation:\n",
        "\n",
        "The `show_examples` function displays a specified number of examples from the dataset:\n",
        "\n",
        "1. `plt.figure(figsize=(15, 3))`: Creates a figure with a width of 15 inches and a height of 3 inches\n",
        "\n",
        "2. For each example:\n",
        "   - Check if the dataset is a `Subset` (which is the case for our training and validation sets after using `random_split`)\n",
        "   - If it is a `Subset`, access the underlying dataset using the indices from the subset\n",
        "   - Otherwise, access the dataset directly\n",
        "   - `plt.subplot(1, num_examples, i+1)`: Creates a subplot in a 1 × num_examples grid at position i+1\n",
        "   - `img.squeeze().numpy()`: Removes dimensions of size 1 and converts the tensor to a NumPy array\n",
        "   - `plt.imshow(..., cmap='gray')`: Displays the image in grayscale\n",
        "   - `plt.title(f'Label: {label}')`: Sets the title to show the label\n",
        "   - `plt.axis('off')`: Hides the x and y axes\n",
        "\n",
        "3. `plt.tight_layout()`: Adjusts the spacing between subplots to avoid overlap\n",
        "\n",
        "4. `plt.show()`: Displays the figure\n",
        "\n",
        "This visualization helps us understand what the MNIST images look like. Each image is a 28×28 grayscale image of a handwritten digit from 0 to 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjXgkuhQWKM1"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "Now, let's define our model. In the context of neural networks, logistic regression is essentially a single-layer neural network with a softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si0uLtUoWKM1"
      },
      "outputs": [],
      "source": [
        "# Define the logistic regression model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)  # Single fully connected layer\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input: reshape from [batch_size, 1, 28, 28] to [batch_size, 784]\n",
        "        x = self.linear(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
        "num_classes = 10       # There are 10 digit classes (0-9)\n",
        "model = LogisticRegressionModel(input_size, num_classes).to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1DHIIzrWKM1"
      },
      "source": [
        "### Model Architecture Explanation:\n",
        "\n",
        "Our logistic regression model is very simple:\n",
        "\n",
        "1. **Model Class**: We define a custom `LogisticRegressionModel` class that inherits from `nn.Module`, which is the base class for all neural networks in PyTorch.\n",
        "\n",
        "2. **Initialization (`__init__`)**:\n",
        "   - `super(LogisticRegressionModel, self).__init__()`: Calls the constructor of the parent class\n",
        "   - `self.linear = nn.Linear(input_size, num_classes)`: Creates a single fully connected (linear) layer that maps from `input_size` (784) to `num_classes` (10)\n",
        "\n",
        "3. **Forward Pass (`forward`)**:\n",
        "   - `x.view(x.size(0), -1)`: Reshapes the input tensor from [batch_size, 1, 28, 28] to [batch_size, 784], effectively flattening the 28×28 images into 784-dimensional vectors\n",
        "   - `self.linear(x)`: Applies the linear transformation, producing raw scores (logits) for each class\n",
        "\n",
        "4. **Model Instantiation**:\n",
        "   - `input_size = 28 * 28`: The number of input features (784)\n",
        "   - `num_classes = 10`: The number of output classes (digits 0-9)\n",
        "   - `model = LogisticRegressionModel(input_size, num_classes).to(device)`: Creates an instance of the model and moves it to the appropriate device (CPU or GPU)\n",
        "\n",
        "This model performs logistic regression by applying a single linear transformation to the flattened input images. The output is a vector of 10 values (logits), one for each digit class. These logits will be converted to probabilities using the softmax function, which is implicitly applied by the cross-entropy loss function we'll use for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RkZm4cDS3owG"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# class TwoLayerNN(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size):\n",
        "#         super(TwoLayerNN, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_size, hidden_size)  # First layer\n",
        "#         self.relu = nn.ReLU()  # Activation function\n",
        "#         self.fc2 = nn.Linear(hidden_size, output_size)  # Second layer\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(-1, 784)  # Flatten the input\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x  # No softmax needed since CrossEntropyLoss includes it\n",
        "\n",
        "# model = TwoLayerNN(28*28, 128, 10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U2uXdY3735Gf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# class CNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CNNModel, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # Conv layer 1\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)  # Conv layer 2\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling\n",
        "#         self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Fully connected layer\n",
        "#         self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.pool(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.pool(x)\n",
        "#         x = x.view(x.size(0), -1)  # Flatten\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x  # No softmax needed since CrossEntropyLoss includes it\n",
        "\n",
        "# model = CNNModel().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_EZARlvWKM1"
      },
      "source": [
        "## Defining Loss Function and Optimizer\n",
        "\n",
        "Now, we need to define:\n",
        "1. A loss function to measure how well the model is performing\n",
        "2. An optimizer to update the model parameters based on the computed gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE0GdhqyWKM1"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.03    # Step size at each iteration while moving toward a minimum of the loss function\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Combines LogSoftmax and NLLLoss in one single class\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # Stochastic Gradient Descent optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0S7erh-WKM1"
      },
      "source": [
        "### Loss Function and Optimizer Explanation:\n",
        "\n",
        "1. **Loss Function (Cross-Entropy Loss)**:\n",
        "   - `nn.CrossEntropyLoss()`: This is a commonly used loss function for multi-class classification problems\n",
        "   - It combines two operations:\n",
        "     - Softmax: Converts the raw model outputs (logits) into probabilities\n",
        "     - Negative Log-Likelihood Loss: Measures the performance of a classification model whose output is a probability value between 0 and 1\n",
        "   - The loss increases as the predicted probability diverges from the actual label\n",
        "\n",
        "2. **Optimizer (Stochastic Gradient Descent)**:\n",
        "   - `optim.SGD(model.parameters(), lr=learning_rate)`: Creates an SGD optimizer that will update the model parameters\n",
        "   - `model.parameters()`: The parameters to optimize (weights and biases of the model)\n",
        "   - `lr=learning_rate`: The learning rate (0.01), which controls the step size during optimization\n",
        "\n",
        "The optimizer will use the gradients computed during backpropagation to update the model parameters in a direction that minimizes the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5YmAi1SWKM2"
      },
      "source": [
        "## Creating an Evaluation Function\n",
        "\n",
        "Let's define a function to evaluate the model on a given dataset. This will be used to evaluate the model on both the validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0_Kqdv5WKM2"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to the appropriate device\n",
        "            outputs = model(images)  # Get model predictions\n",
        "            loss = criterion(outputs, labels)  # Calculate the loss\n",
        "\n",
        "            running_loss += loss.item()  # Accumulate the loss\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n",
        "            total += labels.size(0)  # Count the total number of samples\n",
        "            correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
        "\n",
        "    accuracy = 100 * correct / total  # Accuracy as a percentage\n",
        "    avg_loss = running_loss / len(data_loader)  # Average loss\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWHeCv2YWKM2"
      },
      "source": [
        "### Evaluation Function Explanation:\n",
        "\n",
        "The `evaluate` function assesses the model's performance on a given dataset:\n",
        "\n",
        "1. `model.eval()`: Sets the model to evaluation mode, which disables features like dropout and uses the running statistics for batch normalization (though our simple model doesn't use these)\n",
        "\n",
        "2. `with torch.no_grad()`: Disables gradient calculation, which reduces memory usage and speeds up computation since we don't need gradients for evaluation\n",
        "\n",
        "3. For each batch:\n",
        "   - Move the data to the appropriate device (CPU or GPU)\n",
        "   - Pass the images through the model to get predictions\n",
        "   - Calculate the loss between predictions and true labels\n",
        "   - Accumulate the loss\n",
        "   - Get the predicted class (the index with the highest value)\n",
        "   - Count the total number of samples and the number of correct predictions\n",
        "\n",
        "4. Calculate and return the average loss and accuracy:\n",
        "   - `accuracy = 100 * correct / total`: Accuracy as a percentage\n",
        "   - `avg_loss = running_loss / len(data_loader)`: Average loss over all batches\n",
        "\n",
        "This function allows us to evaluate the model on both the validation set during training and the test set after training is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSilX9MQWKM2"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Now, let's train our model for the specified number of epochs. During training, we'll track the loss and accuracy on both the training and validation sets to monitor the model's progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwybaaUIWKM2"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20         # Number of complete passes through the training dataset\n",
        "\n",
        "# Training loop\n",
        "train_losses = []      # To store the training loss for each epoch\n",
        "train_accuracies = []  # To store the training accuracy for each epoch\n",
        "val_losses = []        # To store the validation loss for each epoch\n",
        "val_accuracies = []    # To store the validation accuracy for each epoch\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:  # Iterate through the training data in batches\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to the appropriate device\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)  # Get model predictions\n",
        "        loss = criterion(outputs, labels)  # Calculate the loss\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update parameters\n",
        "\n",
        "        # Track statistics\n",
        "        running_loss += loss.item()  # Accumulate the loss\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n",
        "        total += labels.size(0)  # Count the total number of samples\n",
        "        correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
        "\n",
        "    # Calculate training metrics\n",
        "    train_loss = running_loss / len(train_loader)  # Average loss for the epoch\n",
        "    train_accuracy = 100 * correct / total  # Accuracy as a percentage\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss, val_accuracy = evaluate(model, val_loader)  # Evaluate on validation set\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0rBDv9rWKM2"
      },
      "source": [
        "### Training Loop Explanation:\n",
        "\n",
        "The training loop iterates through the dataset multiple times (epochs) to train the model:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - `train_losses`, `train_accuracies`, `val_losses`, `val_accuracies`: Lists to store the metrics for each epoch\n",
        "\n",
        "2. **For each epoch**:\n",
        "   \n",
        "   - **Training phase**:\n",
        "     - `model.train()`: Sets the model to training mode\n",
        "     - Initialize counters for loss, correct predictions, and total samples\n",
        "     - For each batch:\n",
        "       - Move the data to the appropriate device (CPU or GPU)\n",
        "       - **Forward pass**: Pass the images through the model and calculate the loss\n",
        "       - **Backward pass and optimization**: Clear gradients, compute gradients, and update parameters\n",
        "       - **Track statistics**: Accumulate loss and count correct predictions\n",
        "     - Calculate training loss and accuracy for the epoch\n",
        "   \n",
        "   - **Validation phase**:\n",
        "     - Call the `evaluate` function on the validation set to get validation loss and accuracy\n",
        "   \n",
        "   - **Store and print metrics**:\n",
        "     - Store the training and validation metrics for later visualization\n",
        "     - Print the statistics for the current epoch\n",
        "\n",
        "This training loop follows the standard pattern for training neural networks with validation: train on the training set, evaluate on the validation set, and track metrics to monitor progress. By monitoring both training and validation metrics, we can detect issues like overfitting (when the model performs well on the training data but poorly on the validation data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbt7gJYbWKM2"
      },
      "source": [
        "## Visualizing Training and Validation Metrics\n",
        "\n",
        "Let's visualize how the loss and accuracy changed during training for both the training and validation sets. This will help us understand the model's learning process and detect any potential issues like overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PraNYRDSWKM2"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation metrics\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training')\n",
        "plt.plot(val_losses, label='Validation')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training')\n",
        "plt.plot(val_accuracies, label='Validation')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjHF4xUOWKM3"
      },
      "source": [
        "### Visualization Explanation:\n",
        "\n",
        "We create two plots side by side to visualize the training and validation metrics:\n",
        "\n",
        "1. **Loss Plot**:\n",
        "   - Shows how the loss decreases over epochs for both training and validation sets\n",
        "   - A decreasing loss indicates that the model is learning and improving its predictions\n",
        "   - If the training loss continues to decrease but the validation loss starts to increase, it may indicate overfitting\n",
        "\n",
        "2. **Accuracy Plot**:\n",
        "   - Shows how the accuracy increases over epochs for both training and validation sets\n",
        "   - An increasing accuracy indicates that the model is making more correct predictions\n",
        "   - If the training accuracy continues to increase but the validation accuracy plateaus or decreases, it may indicate overfitting\n",
        "\n",
        "These plots help us understand the model's learning dynamics and can indicate if the model is converging to a good solution or if there are issues like overfitting or underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKf2NyRFWKM3"
      },
      "source": [
        "## Evaluating the Model on Test Set\n",
        "\n",
        "Finally, let's evaluate our trained model on the test dataset to see how well it generalizes to completely unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKtx8BO4WKM3"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re4rHOsfWKM3"
      },
      "source": [
        "### Test Evaluation Explanation:\n",
        "\n",
        "We use the `evaluate` function we defined earlier to evaluate the model on the test set. This gives us a final measure of how well our model generalizes to completely unseen data.\n",
        "\n",
        "The test set is a separate dataset that was not used during training or validation. It provides an unbiased evaluation of the final model's performance. A good model should have a test accuracy that is close to its validation accuracy. If the test accuracy is significantly lower than the validation accuracy, it might indicate that the validation set was not representative of the general data distribution."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}