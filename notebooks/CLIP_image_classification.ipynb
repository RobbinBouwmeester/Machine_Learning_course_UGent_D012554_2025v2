{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/Machine_Learning_course_UGent_D012554_2025/blob/main/notebooks/CLIP_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SNAYEUBli9P"
      },
      "source": [
        "# Zero-Shot Image Classification with CLIP\n",
        "\n",
        "This notebook demonstrates how to use OpenAI's CLIP model for zero-shot image classification. CLIP (Contrastive Language-Image Pre-training) is a neural network trained on a variety of image-text pairs, allowing it to perform zero-shot predictions on new images without specific training.\n",
        "\n",
        "We'll use the Hugging Face Transformers library to load the model and perform predictions on a local image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUifqWdKli9Q"
      },
      "source": [
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D-mTbTAli9R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DLiyLVMli9R"
      },
      "source": [
        "## Load Model and Processor\n",
        "\n",
        "We'll use the `clip-vit-base-patch32` model from OpenAI, which is available through Hugging Face's model hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucS8RRvili9R"
      },
      "outputs": [],
      "source": [
        "# Load model and processor\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "print(\"Model and processor loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMWBQWz3li9S"
      },
      "source": [
        "## Define Candidate Labels\n",
        "\n",
        "In zero-shot classification, we need to provide the model with potential class descriptions. The model will then determine which description best matches the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MmJVxBqli9S"
      },
      "outputs": [],
      "source": [
        "# Define candidate labels (zero-shot prompts)\n",
        "candidate_labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPIQ5XJQli9S"
      },
      "source": [
        "## Define Classification Function\n",
        "\n",
        "Let's create a function that can classify local images using our model and candidate labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrBEBzPRli9S"
      },
      "outputs": [],
      "source": [
        "def classify_local_image(image_path, labels):\n",
        "    \"\"\"\n",
        "    Classify a local image using the CLIP model with zero-shot learning.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the local image file\n",
        "        labels (list): List of text descriptions for zero-shot classification\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predicted_label, probabilities) where predicted_label is the most likely\n",
        "               label and probabilities is the softmax distribution over all labels\n",
        "    \"\"\"\n",
        "    # Open the local image file\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Process the image and the text inputs\n",
        "    inputs = processor(images=image, text=labels, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the logits (unnormalized predictions)\n",
        "    logits = outputs.logits_per_image\n",
        "\n",
        "    # Convert to probabilities using softmax\n",
        "    probs = logits.softmax(dim=1)\n",
        "\n",
        "    # Find the label with the highest probability\n",
        "    predicted_idx = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    return labels[predicted_idx], probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH2hwEsFli9T"
      },
      "source": [
        "## Visualize the Image\n",
        "\n",
        "Let's display the image before we classify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDtc_P0vli9T"
      },
      "outputs": [],
      "source": [
        "# Replace with the path to your local image\n",
        "local_image_path = \"dog.jpg\"\n",
        "\n",
        "# Display the image\n",
        "image = Image.open(local_image_path)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.title('Image to classify')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s90thw8Bli9T"
      },
      "source": [
        "## Classify the Image\n",
        "\n",
        "Now, let's classify the image and see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tBxmU2yli9T"
      },
      "outputs": [],
      "source": [
        "# Classify the local image\n",
        "predicted_label_local, probabilities_local = classify_local_image(local_image_path, candidate_labels)\n",
        "\n",
        "print(\"Predicted Label:\", predicted_label_local)\n",
        "print(\"Probabilities:\", probabilities_local)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIdGNL8mli9T"
      },
      "source": [
        "## Visualize the Results\n",
        "\n",
        "Let's create a bar chart to visualize the probabilities for each label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-tHD4Y-li9T"
      },
      "outputs": [],
      "source": [
        "# Convert probabilities to a format suitable for plotting\n",
        "probs_list = probabilities_local.squeeze().tolist()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(candidate_labels, probs_list, color='skyblue')\n",
        "\n",
        "# Highlight the predicted label\n",
        "predicted_idx = candidate_labels.index(predicted_label_local)\n",
        "bars[predicted_idx].set_color('navy')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Zero-Shot Classification Results')\n",
        "plt.xticks(rotation=15, ha='right')\n",
        "\n",
        "# Add probability values on top of bars\n",
        "for i, v in enumerate(probs_list):\n",
        "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUrxEsV1li9U"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we demonstrated how to use the CLIP model for zero-shot image classification. This approach allows us to classify images into arbitrary categories without any specific training, by leveraging the model's understanding of both images and text descriptions.\n",
        "\n",
        "Key benefits of this approach:\n",
        "- No need to train or fine-tune models for specific classification tasks\n",
        "- Flexibility to define custom categories on-the-fly\n",
        "- Works reasonably well for common objects and scenes\n",
        "\n",
        "Limitations:\n",
        "- Performance may not match specialized models trained for specific tasks\n",
        "- Results depend heavily on how well the text descriptions match the model's understanding\n",
        "- Computationally more intensive than simple classification models"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}