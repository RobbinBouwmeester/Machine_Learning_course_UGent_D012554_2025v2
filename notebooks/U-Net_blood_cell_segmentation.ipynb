{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/Machine_Learning_course_UGent_D012554_2025/blob/main/notebooks/U-Net_blood_cell_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K_H97lDmbHh"
      },
      "source": [
        "# Blood Cell Segmentation: U-Net\n",
        "\n",
        "This script implements a U-Net architecture for blood cell segmentation using PyTorch.\n",
        "The U-Net is a convolutional neural network that was developed for biomedical image segmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4J7v_6jmbHi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "1pK1UApOmbHj"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVIPDcErmbHj"
      },
      "source": [
        "Download the blood cell dataset with masks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "EHCGaVqKmbHj"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "jeetblahiri_bccd_dataset_with_mask_path = kagglehub.dataset_download('jeetblahiri/bccd-dataset-with-mask')\n",
        "\n",
        "jeetblahiri_bccd_dataset_with_mask_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "Uli1C6lumbHk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from torchvision import io\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "torch.manual_seed = 42\n",
        "np.random.seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8UkeeHmbHk"
      },
      "source": [
        "Collect paths to all training images and their corresponding masks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "q46-YcmYmbHk"
      },
      "outputs": [],
      "source": [
        "root_dir = jeetblahiri_bccd_dataset_with_mask_path + '/BCCD Dataset with mask'\n",
        "\n",
        "train_images = list(Path(f'{root_dir}/train/original').glob('*'))\n",
        "train_masks = list(Path(f'{root_dir}/train/mask').glob('*'))\n",
        "\n",
        "# Collect paths to all test images and their corresponding masks\n",
        "test_images = list(Path(f'{root_dir}/test/original').glob('*'))\n",
        "test_masks = list(Path(f'{root_dir}/test/mask').glob('*'))\n",
        "\n",
        "# Sort the paths to ensure images and masks match correctly\n",
        "train_images.sort()\n",
        "train_masks.sort()\n",
        "test_images.sort()\n",
        "test_masks.sort()\n",
        "\n",
        "# Verify the sorting by displaying paths to the first image and mask\n",
        "str(train_masks[0]), str(train_images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXfw6ZvmbHk"
      },
      "source": [
        "Display a sample image and its mask:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "wZj83f3GmbHl"
      },
      "outputs": [],
      "source": [
        "image_idx = 10\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(io.read_image(str(train_images[0])).permute(1,2,0))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(io.read_image(str(train_masks[0])).permute(1,2,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTmknYx0mbHl"
      },
      "source": [
        "Create a custom dataset class for blood cell images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "T4LnyOuTmbHl"
      },
      "outputs": [],
      "source": [
        "class BloodCellDatase(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for blood cell images and masks.\n",
        "\n",
        "    Attributes:\n",
        "        images (list): List of paths to images\n",
        "        masks (list): List of paths to corresponding masks\n",
        "        transform: Transformations to apply to images and masks\n",
        "    \"\"\"\n",
        "    def __init__(self,images,masks,transform):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.transform = transform\n",
        "\n",
        "    \"\"\"Return the total number of samples in the dataset\"\"\"\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        \"\"\"\n",
        "        Get a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, mask) where both are PyTorch tensors\n",
        "        \"\"\"\n",
        "        image = self.transform(io.read_image(str(self.images[idx])))\n",
        "        mask = self.transform(io.read_image(str(self.masks[idx])))\n",
        "        return image.to(torch.float32), mask[0:1, :, :].to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmW8325HmbHl"
      },
      "source": [
        "Define transformation pipeline and create datasets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "V9JbH7uMmbHl"
      },
      "outputs": [],
      "source": [
        "# Define transformation to resize images and masks to 180x180\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((180,180)),\n",
        "])\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = BloodCellDatase(train_images,train_masks,transform=transform)\n",
        "test_dataset = BloodCellDatase(test_images,test_masks,transform=transform)\n",
        "\n",
        "# Split training data into training and validation sets (80%-20% split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_images) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Display dataset sizes\n",
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIFxPZwmbHm"
      },
      "source": [
        "Visualize a sample from the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "pqb5FPNumbHm"
      },
      "outputs": [],
      "source": [
        "image_idx = 10\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(train_dataset[image_idx][0].permute(1,2,0).numpy().astype(np.uint8))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(train_dataset[image_idx][1].permute(1,2,0).numpy().astype(np.uint8), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX-r6747mbHm"
      },
      "source": [
        "Create data loaders for batch processing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "uhl66CbOmbHm"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with batch size of 64\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKXC6qXpmbHm"
      },
      "source": [
        "Check shapes of images and masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "mbgLB8B3mbHm"
      },
      "outputs": [],
      "source": [
        "train_dataset[100][0].permute(1,2,0).shape, train_dataset[100][1].permute(1,2,0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQK4j4zqmbHn"
      },
      "source": [
        "## The U-Net model architecture\n",
        "\n",
        "First, define the double convolutional block used in U-Net.\n",
        "\n",
        "**Overview of the DoubleConv Block:**\n",
        "  - Implements two consecutive convolutional layers for feature extraction.\n",
        "  - Each convolution is followed by batch normalization and a ReLU activation.\n",
        "\n",
        "**Attributes Defined in the `__init__` Method:**\n",
        "  - **`conv` (nn.Sequential):**\n",
        "    - A sequential container that stacks layers in the following order:\n",
        "      - **First Convolution:**\n",
        "        - `nn.Conv2d` layer transforming input channels to output channels.\n",
        "        - Uses a 3×3 kernel, stride of 1, and padding of 1 to preserve spatial dimensions.\n",
        "        - `bias=False` to rely on batch normalization for bias correction.\n",
        "      - **First Batch Normalization:**\n",
        "        - `nn.BatchNorm2d` applied to the output channels.\n",
        "      - **First Activation:**\n",
        "        - `nn.ReLU` with `inplace=True` for non-linear activation.\n",
        "      - **Second Convolution:**\n",
        "        - Another `nn.Conv2d` layer that processes the already transformed features.\n",
        "        - Keeps the number of channels constant (from output channels to output channels).\n",
        "        - Uses the same kernel size, stride, and padding as the first convolution.\n",
        "      - **Second Batch Normalization:**\n",
        "        - Normalizes the features again to stabilize training.\n",
        "      - **Second Activation:**\n",
        "        - A second `nn.ReLU` activation to introduce additional non-linearity.\n",
        "\n",
        "**Initialization Process:**\n",
        "  - The `__init__` method takes two parameters:\n",
        "    - `in_channels`: Number of channels in the input tensor.\n",
        "    - `out_channels`: Desired number of channels after the convolutional operations.\n",
        "  - These parameters are used to construct the two convolutional blocks within the `nn.Sequential` container.\n",
        "\n",
        "**Forward Pass:**\n",
        "  - The `forward` method applies the sequential block (`self.conv`) to the input tensor `x`.\n",
        "  - Returns the output after processing through both convolutional layers, their corresponding batch normalization, and ReLU activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "jsPvEYF6mbHn"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Double Convolutional block for U-Net architecture.\n",
        "\n",
        "    This block consists of two consecutive convolutional layers\n",
        "    each followed by batch normalization and ReLU activation.\n",
        "\n",
        "    Attributes:\n",
        "        conv (nn.Sequential): The sequence of layers\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Initialize the DoubleConv module.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels\n",
        "            out_channels (int): Number of output channels\n",
        "        \"\"\"\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the double convolution block\"\"\"\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6z-EjpSmbHn"
      },
      "source": [
        "Define the complete U-Net architecture.\n",
        "\n",
        "**Overview of the U-Net Architecture:**\n",
        "  - Designed for image segmentation tasks.\n",
        "  - Uses an encoder-decoder (downsampling-upsampling) structure.\n",
        "  - Incorporates skip connections to fuse high-resolution features from the encoder with upsampled features in the decoder.\n",
        "\n",
        "**Attributes Defined in the `__init__` Method:**\n",
        "  - **`ups` (nn.ModuleList):**\n",
        "    - Stores upsampling layers used in the decoder path.\n",
        "  - **`downs` (nn.ModuleList):**\n",
        "    - Stores downsampling layers (each typically a double convolution block) used in the encoder path.\n",
        "  - **`pool` (nn.MaxPool2d):**\n",
        "    - A max pooling layer with kernel size 2 and stride 2 to reduce spatial dimensions during encoding.\n",
        "  - **`bottleneck` (DoubleConv):**\n",
        "    - A convolutional block at the lowest part of the U, connecting the encoder and decoder.\n",
        "  - **`final_conv` (nn.Conv2d):**\n",
        "    - A 1×1 convolution that maps the final feature maps to the desired number of output channels (e.g., a binary mask).\n",
        "\n",
        "**Initialization Process (`__init__` Method):**\n",
        "  - **Input Parameters:**\n",
        "    - `in_channels`: Number of channels in the input image (default is 3 for RGB images).\n",
        "    - `out_channels`: Number of channels in the output segmentation mask (default is 1 for binary segmentation).\n",
        "    - `features`: A list of integers defining the number of feature maps at each level of the U-Net.\n",
        "  - **Encoder Path Setup:**\n",
        "    - Iterates over the `features` list.\n",
        "    - For each feature value:\n",
        "      - Adds a `DoubleConv` layer (which likely includes two convolutional layers with activations) to the `downs` list.\n",
        "      - Updates `in_channels` to the current feature count for the next layer.\n",
        "  - **Decoder Path Setup:**\n",
        "    - Iterates over the reversed `features` list.\n",
        "    - For each feature value:\n",
        "      - Adds an upsampling layer using `nn.ConvTranspose2d`:\n",
        "        - This layer upsamples the feature maps (doubling the spatial dimensions).\n",
        "        - The number of input channels to this layer is `feature * 2` (because of the concatenation with skip connections later).\n",
        "      - Adds another `DoubleConv` layer to refine the features after concatenation.\n",
        "  - **Bottleneck and Final Convolution:**\n",
        "    - The bottleneck is a `DoubleConv` that processes the most compressed feature representation.\n",
        "    - The `final_conv` layer reduces the number of channels to `out_channels` using a 1×1 convolution, producing the final segmentation mask.\n",
        "\n",
        "**Forward Pass (`forward` Method):**\n",
        "  - **Encoder Phase (Downsampling):**\n",
        "    - Iterates over each module in the `downs` list:\n",
        "      - Applies the `DoubleConv` block.\n",
        "      - Stores the output in a `skip_connections` list for later use.\n",
        "      - Applies max pooling (`self.pool`) to reduce the spatial dimensions before passing to the next block.\n",
        "  - **Bottleneck Processing:**\n",
        "    - Processes the pooled feature map through the `bottleneck` layer to extract deep features.\n",
        "  - **Decoder Phase (Upsampling):**\n",
        "    - Reverses the `skip_connections` list so that the last encoded features are used first in the decoder.\n",
        "    - Processes the feature maps in pairs (upsampling layer followed by a `DoubleConv`):\n",
        "      - **Upsampling:**\n",
        "        - Uses the `nn.ConvTranspose2d` layer to upsample the feature maps.\n",
        "      - **Skip Connection:**\n",
        "        - Retrieves the corresponding encoder output.\n",
        "        - If there is a mismatch in shape, the upsampled feature map is resized to match the skip connection.\n",
        "      - **Concatenation & Refinement:**\n",
        "        - Concatenates the skip connection with the upsampled features along the channel dimension.\n",
        "        - Applies the subsequent `DoubleConv` layer to fuse and refine the combined features.\n",
        "  - **Final Output:**\n",
        "    - Passes the refined feature maps through the `final_conv` (1×1 convolution) to produce the final segmentation mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "vqzHOMyymbHn"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for image segmentation.\n",
        "\n",
        "    The U-Net consists of an encoder (downsampling) path and\n",
        "    a decoder (upsampling) path with skip connections between them.\n",
        "\n",
        "    Attributes:\n",
        "        ups (nn.ModuleList): List of upsampling layers\n",
        "        downs (nn.ModuleList): List of downsampling layers\n",
        "        pool (nn.MaxPool2d): Pooling layer for downsampling\n",
        "        bottleneck (DoubleConv): The bottleneck layer at the base of the \"U\"\n",
        "        final_conv (nn.Conv2d): Final 1x1 convolution to produce output\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the U-Net model.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels (default: 3 for RGB images)\n",
        "            out_channels (int): Number of output channels (default: 1 for binary mask)\n",
        "            features (list): List of feature dimensions for each level of the U-Net\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET (encoder path)\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET (decoder path)\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        # Bottleneck at the bottom of the U\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        # Final 1x1 convolution to produce output mask\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the U-Net model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input image tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output segmentation mask\n",
        "        \"\"\"\n",
        "        # Store skip connections\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encoder path (downsampling)\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        # Reverse skip connections for decoder path\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        # Decoder path (upsampling) with skip connections\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            # Resize if shapes don't match\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            # Concatenate skip connection with upsampled feature maps\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        # Final convolution\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dduluivmbHo"
      },
      "source": [
        "Train the U-Net model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "68CCrrdCmbHo"
      },
      "outputs": [],
      "source": [
        "# Determine device (GPU or CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Define training parameters\n",
        "epoches = 3\n",
        "# Initialize the model and move to device\n",
        "model = UNet().to(device)\n",
        "# Set optimizer with learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "# Define loss function - Binary Cross Entropy with Logits\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epoches):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Loop through batches\n",
        "    for images, masks in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to device\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(outputs, masks)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        train_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    # Calculate and print average loss for the epoch\n",
        "    avg_loss = (train_loss/num_batches)\n",
        "    print(f'Epoch {epoch+1}/{epoches} loss: {avg_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgJzOAaYmbHo"
      },
      "source": [
        "Model inference and visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "THAfOOo9mbHo"
      },
      "outputs": [],
      "source": [
        "def predict_mask(model, image):\n",
        "    \"\"\"\n",
        "    Generate a segmentation mask prediction for a single image.\n",
        "\n",
        "    Args:\n",
        "        model: Trained U-Net model\n",
        "        image (torch.Tensor): Input image tensor\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Predicted mask\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        return model(image.unsqueeze(0)).squeeze(0).squeeze(0).to('cpu')\n",
        "\n",
        "# Select an image from the test dataset\n",
        "image_idx = 17\n",
        "\n",
        "# Get the image and its ground truth mask\n",
        "x, y = test_dataset[image_idx]\n",
        "# Generate prediction\n",
        "y_pred = predict_mask(model, x)\n",
        "\n",
        "# Convert tensors to numpy arrays for visualization\n",
        "image = x.permute(1,2,0).detach().numpy().astype(np.uint8)\n",
        "mask = y.permute(1,2,0).detach().numpy().astype(np.uint8)\n",
        "y_mask = y_pred.detach().numpy().astype(np.uint8)\n",
        "\n",
        "# Visualize the original image, ground truth mask, and predicted mask\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Input Image')\n",
        "plt.imshow(image)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Actual Mask')\n",
        "plt.imshow(mask, cmap='gray')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Predicted Mask')\n",
        "plt.imshow(1 - y_mask, cmap='gray')"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 6186133,
          "datasetId": 3498826,
          "sourceId": 6107556,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}