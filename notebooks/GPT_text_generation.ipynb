{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHpWhHXHe2S9"
      },
      "source": [
        "# Text Generation with GPT Models\n",
        "\n",
        "This notebook demonstrates how to generate text using pre-trained GPT models from Hugging Face, with visualizations of token probabilities and various generation parameters.\n",
        "\n",
        "## Overview\n",
        "- We'll use HuggingFace's transformers library to load a GPT model\n",
        "- The notebook shows how to control text generation using parameters like temperature, top-k, and top-p\n",
        "- We include an option to visualize the token-by-token generation process and probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOJnz8Bke2TA"
      },
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6_wkbQle2TA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4uxEaqe2TB"
      },
      "source": [
        "## 2. Define Text Generation Function\n",
        "\n",
        "The function below handles text generation with various parameters:\n",
        "- `temperature`: Controls randomness (lower = more deterministic)\n",
        "- `top_k`: Limits token selection to the top k highest probability tokens\n",
        "- `top_p`: Uses nucleus sampling to select tokens whose cumulative probability exceeds threshold p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdY6DLAWe2TB"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    prompt,\n",
        "    model_name=\"gpt2\",  # Using GPT-2 as a stand-in for GPT-3 since GPT-3 isn't directly available in HF\n",
        "    max_length=100,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate text using a GPT model from Hugging Face with temperature control\n",
        "    and visualization of token probabilities.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input text to start generation from\n",
        "        model_name (str): Name of the model to use from Hugging Face\n",
        "        max_length (int): Maximum length of the generated text\n",
        "        temperature (float): Controls randomness. Lower means more deterministic\n",
        "        top_k (int): Number of highest probability tokens to consider\n",
        "        top_p (float): Cumulative probability cutoff for token selection\n",
        "        num_return_sequences (int): Number of sequences to return\n",
        "\n",
        "    Returns:\n",
        "        list: Generated text sequences\n",
        "    \"\"\"\n",
        "    # Load model and tokenizer\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids\n",
        "\n",
        "    # Store original input length to track new tokens\n",
        "    input_length = input_ids.shape[1]\n",
        "\n",
        "    # Generate with step-by-step probability display\n",
        "    print(f\"\\nGenerating text with temperature={temperature}, top_k={top_k}, top_p={top_p}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(\"\\nGeneration Process:\")\n",
        "\n",
        "    # Start with the input sequence\n",
        "    current_ids = input_ids.clone()\n",
        "    generated_tokens = []\n",
        "\n",
        "    # Generate one token at a time\n",
        "    for _ in range(max_length - input_length):\n",
        "        with torch.no_grad():\n",
        "            # Get model outputs\n",
        "            outputs = model(current_ids)\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = torch.topk(next_token_logits, k=top_k)[0][..., -1, None]\n",
        "                next_token_logits[next_token_logits < indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Apply top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Remove tokens with cumulative probability above the threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                # Shift the indices to the right to keep the first token above the threshold\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                next_token_logits[0, indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "            probs = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Show top 5 probabilities for the generated token\n",
        "            top_probs, top_indices = torch.topk(probs, k=5)\n",
        "            top_probs = top_probs.squeeze().cpu().numpy()\n",
        "            top_indices = top_indices.squeeze().cpu().numpy()\n",
        "\n",
        "            next_token_text = tokenizer.decode(next_token[0])\n",
        "            generated_tokens.append(next_token_text)\n",
        "\n",
        "            print(f\"\\nGenerated token: '{next_token_text}'\")\n",
        "            print(\"Top 5 token probabilities:\")\n",
        "            for i in range(5):\n",
        "                token = tokenizer.decode([top_indices[i]])\n",
        "                print(f\"  {token}: {top_probs[i]:.6f}\")\n",
        "\n",
        "            # Add to current sequence\n",
        "            current_ids = torch.cat([current_ids, next_token], dim=1)\n",
        "\n",
        "            # Stop if we generate an EOS token\n",
        "            if next_token[0, 0].item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Show the complete generated text\n",
        "    full_text = tokenizer.decode(current_ids[0], skip_special_tokens=True)\n",
        "    print(f\"\\nFinal generated text: {full_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHearoe2e2TC"
      },
      "source": [
        "Let's run our function with a sample prompt to see it in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAEy2Rtfe2TD"
      },
      "outputs": [],
      "source": [
        "prompt = \"In a world where AI has become sentient,\"\n",
        "generate_text(\n",
        "    prompt=prompt,\n",
        "    temperature=0.7,\n",
        "    max_length=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxCleIwAe2TD"
      },
      "source": [
        "### Temperature\n",
        "- Controls the randomness of predictions\n",
        "- Lower temperature (e.g., 0.2) makes the model more confident and deterministic\n",
        "- Higher temperature (e.g., 1.5) makes output more random and creative\n",
        "- At temperature=0, the model always picks the most likely next token\n",
        "\n",
        "### Top-k Sampling\n",
        "- Only considers the top k most likely next tokens\n",
        "- Prevents the model from selecting highly improbable tokens\n",
        "- Common values range from 20-50\n",
        "\n",
        "### Top-p (Nucleus) Sampling\n",
        "- Only considers the smallest set of tokens whose cumulative probability exceeds p\n",
        "- Dynamically adjusts the number of tokens considered based on probability distribution\n",
        "- Typical values range from 0.7-0.95\n",
        "\n",
        "These parameters can be combined to achieve a good balance between coherence and creativity in the generated text."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}