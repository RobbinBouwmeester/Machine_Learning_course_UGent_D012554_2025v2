{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/Machine_Learning_course_UGent_D012554_2025/blob/main/notebooks/overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX1STA1bnp44"
      },
      "source": [
        "# Decision Tree Overfitting Demonstration\n",
        "\n",
        "This notebook demonstrates how decision trees can achieve 100% training accuracy but perform poorly on test data due to overfitting. We'll visualize this phenomenon using a synthetic non-linear dataset (half-moons) and explore how regularization techniques like limiting tree depth can help mitigate overfitting.\n",
        "\n",
        "## What is Overfitting?\n",
        "\n",
        "Overfitting occurs when a model learns the training data too well, capturing noise and outliers rather than just the underlying pattern. This results in:\n",
        "- Excellent performance on training data\n",
        "- Poor performance on new, unseen data (test data)\n",
        "\n",
        "Decision trees are particularly prone to overfitting because they can create very complex decision boundaries that perfectly separate the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmJEURucnp46"
      },
      "source": [
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXw6eMpHnp46"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMsmjQn-np47"
      },
      "source": [
        "## Data Generation\n",
        "\n",
        "We'll generate a synthetic non-linear dataset using scikit-learn's `make_moons` function. This creates two interleaving half-moons, which is a classic example of data that requires non-linear decision boundaries.\n",
        "\n",
        "To make overfitting more pronounced, we'll also add some noise and outliers by flipping the labels of a few random points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9K2oe2Dnp47"
      },
      "outputs": [],
      "source": [
        "# Generate a non-linear dataset (half-moons)\n",
        "X, y = make_moons(n_samples=500, noise=0.3, random_state=1)\n",
        "\n",
        "# Add some noise/outliers to make overfitting more pronounced\n",
        "# Add 20 random outliers\n",
        "n_outliers = 20\n",
        "outlier_indices = np.random.choice(len(X), n_outliers, replace=False)\n",
        "# Flip the labels for these outliers\n",
        "y[outlier_indices] = 1 - y[outlier_indices]\n",
        "\n",
        "# Let's visualize our dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#00FF00']), edgecolor='k', s=20)\n",
        "plt.title('Half-moons Dataset with Outliers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEonmTvjnp47"
      },
      "source": [
        "## Data Splitting\n",
        "\n",
        "We'll split our data into training (70%) and testing (30%) sets. The training set will be used to train our models, while the testing set will be used to evaluate their performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DyioXsknp48"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YSFpKEcnp48"
      },
      "source": [
        "## Training an Unrestricted Decision Tree\n",
        "\n",
        "First, let's train a decision tree with default parameters. By default, scikit-learn's `DecisionTreeClassifier` will grow a tree until all leaves are pure (contain samples of only one class) or until all leaves contain less than `min_samples_split` samples.\n",
        "\n",
        "This allows the tree to potentially grow very deep and create a complex decision boundary that perfectly fits the training data, including noise and outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBbfAymXnp48"
      },
      "outputs": [],
      "source": [
        "# Create a decision tree classifier with default parameters\n",
        "# This allows tree to grow fully and potentially overfit\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Training and testing accuracy\n",
        "train_pred = tree_clf.predict(X_train)\n",
        "test_pred = tree_clf.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "test_accuracy = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Testing accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnIlYsq1np48"
      },
      "source": [
        "## Training a Regularized Decision Tree\n",
        "\n",
        "Now, let's train a regularized decision tree by limiting its maximum depth to 3. This prevents the tree from creating an overly complex decision boundary and forces it to focus on the most important patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUkWuve-np48"
      },
      "outputs": [],
      "source": [
        "# Create a regularized tree with limited depth for comparison\n",
        "reg_tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "reg_tree_clf.fit(X_train, y_train)\n",
        "\n",
        "reg_train_pred = reg_tree_clf.predict(X_train)\n",
        "reg_test_pred = reg_tree_clf.predict(X_test)\n",
        "\n",
        "reg_train_accuracy = accuracy_score(y_train, reg_train_pred)\n",
        "reg_test_accuracy = accuracy_score(y_test, reg_test_pred)\n",
        "\n",
        "print(f\"Regularized tree:\")\n",
        "print(f\"Training accuracy: {reg_train_accuracy:.4f}\")\n",
        "print(f\"Testing accuracy: {reg_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arpJx0brnp49"
      },
      "source": [
        "## Visualizing Decision Boundaries\n",
        "\n",
        "Let's define a function to visualize the decision boundaries of our models. This will help us see how the unrestricted tree creates a complex boundary that perfectly fits the training data, while the regularized tree creates a simpler boundary that generalizes better to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qldLRasInp49"
      },
      "outputs": [],
      "source": [
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundary(clf, X, y, title, ax=None):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Define bounds of the plot\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "\n",
        "    # Create a mesh grid\n",
        "    h = 0.02  # Step size\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Predict on the mesh grid\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Create custom colormap\n",
        "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
        "    cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
        "\n",
        "    # Plot decision boundary and points\n",
        "    ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_title(title)\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wLYWLpjnp49"
      },
      "source": [
        "Now, let's compare the decision boundaries of our unrestricted and regularized trees:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnun0weHnp49"
      },
      "outputs": [],
      "source": [
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot decision boundaries\n",
        "plot_decision_boundary(tree_clf, X_train, y_train,\n",
        "                       f\"Overfitted Tree\\nTrain: {train_accuracy:.4f}, Test: {test_accuracy:.4f}\", ax1)\n",
        "plot_decision_boundary(reg_tree_clf, X_train, y_train,\n",
        "                       f\"Regularized Tree (max_depth=3)\\nTrain: {reg_train_accuracy:.4f}, Test: {reg_test_accuracy:.4f}\", ax2)\n",
        "\n",
        "ax1.legend()\n",
        "ax2.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo2MP_Hznp49"
      },
      "source": [
        "## The Effect of Tree Depth on Overfitting\n",
        "\n",
        "Let's explore how the maximum depth of the tree affects its performance on training and testing data. We'll train multiple trees with different maximum depths and plot their accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTTJCH0Rnp49"
      },
      "outputs": [],
      "source": [
        "# Parameter values to try\n",
        "max_depths = range(1, 21)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for depth in max_depths:\n",
        "    # Train tree with specific depth\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt.fit(X_train, y_train)\n",
        "\n",
        "    # Record accuracies\n",
        "    train_scores.append(accuracy_score(y_train, dt.predict(X_train)))\n",
        "    test_scores.append(accuracy_score(y_test, dt.predict(X_test)))\n",
        "\n",
        "# Plot accuracies vs tree depth\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depths, train_scores, 'o-', color='r', label='Training accuracy')\n",
        "plt.plot(max_depths, test_scores, 'o-', color='g', label='Testing accuracy')\n",
        "\n",
        "plt.axhline(y=1.0, color='r', linestyle=':', alpha=0.5, label='Perfect accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Tree Depth')\n",
        "plt.title('Decision Tree Performance vs Tree Depth')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Mark the point where overfitting starts to become severe\n",
        "best_test_idx = np.argmax(test_scores)\n",
        "plt.scatter([max_depths[best_test_idx]], [test_scores[best_test_idx]],\n",
        "            s=200, facecolors='none', edgecolors='blue', linewidth=2,\n",
        "            label=f'Best test accuracy at depth={max_depths[best_test_idx]}')\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}